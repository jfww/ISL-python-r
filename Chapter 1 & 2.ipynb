{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 1 is mostly introductory, not very technical.\n",
    "\n",
    "*Supervised* vs *Un-supervised* learning:\n",
    "* Supervised learning involves creating a statistical for predicting an *output* given one or more *inputs*.\n",
    "* With un-supervised learning, we still have *inputs* but no supervising output. See *clustering*.\n",
    "\n",
    "*Regression* vs *Classification*\n",
    "* Regression involves predicting a *continuous* or *quantative* value\n",
    "* Classification involves predicting a categorical or *qualitative* value.*\n",
    "\n",
    "Notation:\n",
    "* $n$ = number of observations\n",
    "* $p$ = number of variables\n",
    "* $xij$ = the value of the $jth$ variable for the $ith$ observation. Where $i = 1,2,...n$ and $j = 1,2,...,p$.\n",
    "* $X = n x p$ matrix\n",
    "* $y$ = target variable vector\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input variables synonyms: predictors, indepedent ariables, features\n",
    "* output variable synonyms: respones, dependent variable, target variable\n",
    "\n",
    "Given a quantative response $Y$ and $p$ different predictors $X1,X2,...Xp$ we write the relationship between these predictors and $Y$ as\n",
    "\n",
    "$Y = f(X)+e$\n",
    "\n",
    "Where f represents the *systemic* information that $X$ provides us about $Y$, and $e$ is the *error term* which is independent of $X$ and has a mean of zero.\n",
    "The TRUE function $f$ is generally not known in practice, and so statistical learning is ultimately a set of approaches for estimating $f$\n",
    "\n",
    "Goal is to find a function $\\hat{f}$ so that $Y \\approx f(X)$ for all observations $(X,Y)$.\n",
    "\n",
    "**Prediction vs Inference**\n",
    "\n",
    "* When we have a lot of inputs X but no easy way of obtaining the true output Y, we can predict the outcome: $\\hat{Y}$ = $\\hat{f}$(X) where $\\hat{f}$ is our estimate for f. In this case we are generally mostly interested in as accurate an output of $\\hat{Y}$ as possible and $\\hat{f}$ is treated as a black box.\n",
    "\n",
    "*Example*\n",
    "* Given patient features $X1,...Xp$ and variable $Y$ representing said patient's risk of adverse reaction to a particular drug, we would want to predict $Y$ using $X$, avoiding the risk of an adverse reaction.\n",
    "\n",
    "**Reducive vs irreducible errors**\n",
    "\n",
    "* Even if we could predict the actual function $f$ perfectly, such that $\\hat{Y}$ = $f(X)$, we would still be left with an error, as the actual $Y$ is a function both of $f(X)$ as well as the error term, $e$. This is the irreducible error, independent from X and so we cannot do anything about it.\n",
    "* The reducible error can be minimized through utilizing the appropriate statistical learning techniques.\n",
    "\n",
    "Given an estimate $\\hat{f}$ and a set of predictors $X$, yielding the prediction $\\hat{Y} = \\hat{f}(X)$. Assuming for now that $\\hat{f}, X$ are both fixed, then\n",
    "\n",
    "$E(Y-\\hat{Y})^2 = E[f(X)+e - \\hat{f}(X)]^2$  \n",
    "*< expected value of squared difference between $\\hat{Y}$ and $Y$, and var is the $var(e)$ is the variance of the error term $e$*\n",
    "\n",
    "$= [f(X)-\\hat{f}(X)]^2 + Var(e)$\n",
    "\n",
    "    Reducible | irreducible\n",
    "    \n",
    "The irreducible error provides an upperbound on how accurate any model we make can be.\n",
    "       \n",
    "**Inference**\n",
    "* Understanding how $X$ influences $Y$. \n",
    "* $f$ cannot be treated as a black box here, interpretability is important. \n",
    "* Understanding which variables in $X$ have the most impact on $Y$, and whether their relationship is positive or negative.\n",
    "* Can we model it as a linear relationship? Much easier for interpretation but often not realistic.\n",
    "\n",
    "**Parametric vs non-parametric**\n",
    "\n",
    "*Parametric methods*\n",
    "Parametric methods can be broken into 2 steps:\n",
    "1. We make an assumption about the functional form of $f$, for example that it $f$ is linear in X:\n",
    "$f(X) = \\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p$\n",
    "\n",
    "2. We *fit* (or *train*) our model using the training data. Using the linear model from above as an example we want to find the values of the paramters $\\beta_0,\\beta_1,...\\beta_p$ such that\n",
    "$Y \\approx \\beta_0+\\beta_1X_1+\\beta_2X2+...+\\beta_pX_p$\n",
    "We measure the goodness of our fit using, most commonly, *ordinary least squares*.\n",
    "\n",
    "Pros:\n",
    "* Reduces the possibility space, greatly simplifying the problem.\n",
    "* More interpretable.\n",
    "\n",
    "Cons:\n",
    "* Unlikely to match the true function $f$. If the true function $f$ is highly non-linear, we might want to use a more flexible model.\n",
    "\n",
    "*Non-parametric methods*\n",
    "* Makes no assumption about the functional form of $f$.\n",
    "* Tries to find the smoothest fit possible - i.e find $\\hat{f}$ such that every data point is as close as possible without roughness or wigglyness.\n",
    "* As such, can fit a wider range of shapes for $f$.\n",
    "\n",
    "Pros:\n",
    "* Flexible, can fit a wide range of shapes for $f$.\n",
    "\n",
    "Cons:\n",
    "* As we don't reduce the size of the problem at all, we require many more observations to accurately estimate $f$ than a parametric method would.\n",
    "* Possible to overfit - matching the training data perfectly but not generalizing well to test or new data.\n",
    "* Less interpretable.\n",
    "\n",
    "**Supervised vs Unsupervised**\n",
    "\n",
    "*Supervised*\n",
    "* For every observation of our predictor measurements $x_i, i= 1,...,n$ we have an associated response measurement $yi$.\n",
    "\n",
    "*Unsupervised*\n",
    "* No response variable, simply a set of predictor measurements $x_i, i = 1,...,n$.\n",
    "* Example: clustering, such as with $K$*-means*.\n",
    "\n",
    "**Classification vs Regression**\n",
    "Variables can either be *quantitative* or *qualitiative*. A quantiative variable is a numerical variable (discrete or continuous), while a qualiatitve variable is a categorical.\n",
    "* Ex age is a quantitative variable while gender would be a qualitative variable.\n",
    "\n",
    "Generally speaking a problem with a quantitiative response variable is a regression problem, while one with a qualitative one is a classification problem. There are however many exceptions (logistic regression is often used for classification but as the name implies, is a regression method).\n",
    "\n",
    "The nature of the response variable is a key factor in picking which method to use, but the nature of the features (predictor variables) matters less, generally.\n",
    "\n",
    "**Model Accuracy**\n",
    "\n",
    "*Mean Squared Error*\n",
    "The most common metric for measuring goodness of fit in the regression setting is the MSE, defined as\n",
    "$MSE = 1/n \\sum_{i=1}^{n}(y_i-\\hat{f}(x_i))^2$\n",
    "\n",
    "* Goal is not to minimize training set MSE (doing this would just lead to overfitting).\n",
    "Rather we want to minimize test-set MSE, measuring our model's predictive powers on data not seen during training.\n",
    "\n",
    "$Ave(y_o-\\hat{f}(x_o))$ where $(x_o,y_o)$ are here to unseen by our model.\n",
    "\n",
    "As flexibility of a method increases, training set MSE will decrease, but this does not guarantee that test set MSE will. A big discrepancy in the form of low training set MSE with a high test set MSE is a symptom of overfitting.\n",
    "Note that we are only said to be overfit if a less flexible model would have yielded a better result than the more flexible one.\n",
    "\n",
    "**The Bias-Variance trade-off**\n",
    "\n",
    "Expected test MSE, for a given value $x_o$ can be decomposed as three terms:\n",
    "\n",
    "* The variance of $\\hat{f}(x_o)$\n",
    "* The squared bias of $\\hat{f}(x_o)$\n",
    "* The variance of the error term $e$\n",
    "\n",
    "or\n",
    "$E(y_o-\\hat{f}(x_o)^2 = Var(\\hat{f}(x_o)) + [Bias(\\hat{f}(x_o))]^2 + Var(e)$\n",
    "\n",
    "Overall expected test MSE can be computed by averaging $E(y_o-\\hat{f}(x_o))^2$ over all possible values $x_o$ in the test set.\n",
    "\n",
    "Thus we want to select the method giving us the lowest variance and the lowest bias.\n",
    "Neither value can ever be negative (as we are using squared bias), so we are bounded below in terms of a minimum MSE, on the value of $e$, our *irreducible error* term.\n",
    "\n",
    "Variance here is a measure of how much $\\hat{f}$ would change with a changes to our training set. A very flexible method generally has higher variance than a less flexible one, and would vary more between training sets.\n",
    "\n",
    "Bias on the other hand is how much of an additional error we introduce in our assumptions (for example the assumption of a linear relationship in linear regression). \n",
    "\n",
    "Generally, more flexible = lower bias, higher variance, less flexible = higher bias, lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
